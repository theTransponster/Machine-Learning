{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = 'train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'data2'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total training cat images:', 1000)\n",
      "('total training dog images:', 1000)\n",
      "('total validation cat images:', 500)\n",
      "('total validation dog images:', 500)\n",
      "('total test cat images:', 500)\n",
      "('total test dog images:', 500)\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode=None)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 6s 0us/step\n",
      "58900480/58889256 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   ...\n   [0.22352943 0.21960786 0.20000002]\n   [0.21960786 0.21568629 0.19607845]\n   [0.21568629 0.21176472 0.19215688]]\n\n  [[0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   ...\n   [0.19607845 0.20000002 0.1764706 ]\n   [0.19607845 0.20000002 0.1764706 ]\n   [0.19607845 0.20000002 0.1764706 ]]\n\n  [[0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   ...\n   [0.18431373 0.18823531 0.16862746]\n   [0.17254902 0.1764706  0.15686275]\n   [0.17254902 0.1764706  0.15686275]]\n\n  ...\n\n  [[0.6862745  0.64705884 0.60784316]\n   [0.6862745  0.64705884 0.60784316]\n   [0.6862745  0.64705884 0.60784316]\n   ...\n   [0.29803923 0.35686275 0.38431376]\n   [0.2901961  0.34117648 0.37254903]\n   [0.28627452 0.3372549  0.36862746]]\n\n  [[0.68235296 0.6431373  0.6039216 ]\n   [0.6862745  0.64705884 0.60784316]\n   [0.6862745  0.64705884 0.60784316]\n   ...\n   [0.30588236 0.3647059  0.3921569 ]\n   [0.29411766 0.34509805 0.37647063]\n   [0.27450982 0.3254902  0.35686275]]\n\n  [[0.6862745  0.64705884 0.60784316]\n   [0.68235296 0.6431373  0.6039216 ]\n   [0.68235296 0.6431373  0.6039216 ]\n   ...\n   [0.2901961  0.34901962 0.37647063]\n   [0.28235295 0.33333334 0.3647059 ]\n   [0.2784314  0.32941177 0.36078432]]]\n\n\n [[[0.10196079 0.10196079 0.09411766]\n   [0.10980393 0.10980393 0.10196079]\n   [0.09803922 0.09803922 0.09019608]\n   ...\n   [0.39607847 0.25490198 0.12156864]\n   [0.4039216  0.2627451  0.12941177]\n   [0.427451   0.25882354 0.1254902 ]]\n\n  [[0.09803922 0.09803922 0.09019608]\n   [0.10588236 0.10588236 0.09803922]\n   [0.09803922 0.09803922 0.09019608]\n   ...\n   [0.427451   0.27450982 0.14509805]\n   [0.43529415 0.28235295 0.15294118]\n   [0.454902   0.2784314  0.14901961]]\n\n  [[0.09411766 0.09411766 0.08627451]\n   [0.09803922 0.09803922 0.09019608]\n   [0.09803922 0.09803922 0.09019608]\n   ...\n   [0.454902   0.28627452 0.14901961]\n   [0.45882356 0.2901961  0.15294118]\n   [0.47450984 0.29411766 0.15294118]]\n\n  ...\n\n  [[0.06666667 0.05882353 0.07058824]\n   [0.18039216 0.1764706  0.16862746]\n   [0.5686275  0.5411765  0.4784314 ]\n   ...\n   [0.5568628  0.43137258 0.24705884]\n   [0.5568628  0.43137258 0.24705884]\n   [0.5529412  0.43137258 0.24705884]]\n\n  [[0.06666667 0.05882353 0.07058824]\n   [0.18039216 0.1764706  0.16862746]\n   [0.56078434 0.53333336 0.47058827]\n   ...\n   [0.5568628  0.43137258 0.24705884]\n   [0.5568628  0.43137258 0.24705884]\n   [0.5529412  0.43137258 0.24705884]]\n\n  [[0.06666667 0.05882353 0.07058824]\n   [0.18039216 0.1764706  0.16862746]\n   [0.5529412  0.5254902  0.46274513]\n   ...\n   [0.5568628  0.43137258 0.24705884]\n   [0.5568628  0.43137258 0.24705884]\n   [0.5529412  0.43137258 0.24705884]]]\n\n\n [[[0.78823537 0.79215693 0.8000001 ]\n   [0.7960785  0.8000001  0.8078432 ]\n   [0.7843138  0.78823537 0.7960785 ]\n   ...\n   [0.73333335 0.74509805 0.76470596]\n   [0.74509805 0.7490196  0.7686275 ]\n   [0.74509805 0.7490196  0.7686275 ]]\n\n  [[0.7803922  0.7843138  0.79215693]\n   [0.7686275  0.7725491  0.7803922 ]\n   [0.78823537 0.79215693 0.8000001 ]\n   ...\n   [0.76470596 0.77647066 0.7960785 ]\n   [0.7568628  0.7607844  0.7803922 ]\n   [0.7568628  0.7607844  0.7803922 ]]\n\n  [[0.79215693 0.7960785  0.80392164]\n   [0.7960785  0.8000001  0.8078432 ]\n   [0.79215693 0.7960785  0.80392164]\n   ...\n   [0.73333335 0.74509805 0.76470596]\n   [0.7372549  0.7411765  0.7607844 ]\n   [0.7607844  0.76470596 0.7843138 ]]\n\n  ...\n\n  [[0.8352942  0.83921576 0.8196079 ]\n   [0.82745105 0.8313726  0.8000001 ]\n   [0.8352942  0.8431373  0.79215693]\n   ...\n   [0.7960785  0.81568635 0.8000001 ]\n   [0.7372549  0.7372549  0.7294118 ]\n   [0.7960785  0.7960785  0.78823537]]\n\n  [[0.85098046 0.85098046 0.8588236 ]\n   [0.85098046 0.85098046 0.8431373 ]\n   [0.85098046 0.8588236  0.81568635]\n   ...\n   [0.79215693 0.8117648  0.7960785 ]\n   [0.79215693 0.79215693 0.7843138 ]\n   [0.8078432  0.8078432  0.8000001 ]]\n\n  [[0.81568635 0.8196079  0.78823537]\n   [0.8235295  0.82745105 0.80392164]\n   [0.86666673 0.86666673 0.8588236 ]\n   ...\n   [0.8078432  0.7843138  0.7843138 ]\n   [0.81568635 0.8117648  0.7960785 ]\n   [0.8000001  0.7960785  0.7803922 ]]]\n\n\n ...\n\n\n [[[0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   ...\n   [0.04705883 0.01568628 0.00784314]\n   [0.04313726 0.01176471 0.00392157]\n   [0.04705883 0.         0.        ]]\n\n  [[0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   ...\n   [0.03921569 0.         0.        ]\n   [0.03529412 0.         0.        ]\n   [0.04705883 0.         0.        ]]\n\n  [[0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   ...\n   [0.0509804  0.01176471 0.00784314]\n   [0.04705883 0.00784314 0.00392157]\n   [0.04705883 0.         0.        ]]\n\n  ...\n\n  [[0.41176474 0.38823533 0.38823533]\n   [0.32156864 0.29803923 0.29803923]\n   [0.38823533 0.36862746 0.35686275]\n   ...\n   [0.58431375 0.62352943 0.5882353 ]\n   [0.5921569  0.6156863  0.6       ]\n   [0.5254902  0.5411765  0.5529412 ]]\n\n  [[0.36078432 0.34117648 0.32941177]\n   [0.2784314  0.25490198 0.25490198]\n   [0.36078432 0.3372549  0.34509805]\n   ...\n   [0.59607846 0.63529414 0.5921569 ]\n   [0.5882353  0.6039216  0.6       ]\n   [0.5411765  0.5529412  0.57254905]]\n\n  [[0.40784317 0.38823533 0.37254903]\n   [0.35686275 0.33333334 0.33333334]\n   [0.36862746 0.34509805 0.36078432]\n   ...\n   [0.5764706  0.6039216  0.57254905]\n   [0.53333336 0.5372549  0.5529412 ]\n   [0.47450984 0.48627454 0.5137255 ]]]\n\n\n [[[0.8196079  0.8235295  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   ...\n   [0.7686275  0.7568628  0.63529414]\n   [0.76470596 0.75294125 0.6313726 ]\n   [0.76470596 0.75294125 0.6313726 ]]\n\n  [[0.8235295  0.8196079  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   ...\n   [0.7686275  0.7568628  0.63529414]\n   [0.7686275  0.7568628  0.63529414]\n   [0.76470596 0.75294125 0.6313726 ]]\n\n  [[0.8235295  0.8196079  0.7019608 ]\n   [0.8235295  0.8196079  0.7019608 ]\n   [0.8235295  0.8196079  0.7019608 ]\n   ...\n   [0.7686275  0.7568628  0.63529414]\n   [0.7686275  0.7568628  0.63529414]\n   [0.7686275  0.7568628  0.63529414]]\n\n  ...\n\n  [[0.35686275 0.25882354 0.17254902]\n   [0.37254903 0.27450982 0.18823531]\n   [0.41960788 0.31764707 0.227451  ]\n   ...\n   [0.654902   0.6        0.54901963]\n   [0.654902   0.6        0.54901963]\n   [0.65882355 0.6039216  0.5529412 ]]\n\n  [[0.35686275 0.25882354 0.17254902]\n   [0.3372549  0.2392157  0.15294118]\n   [0.36078432 0.25882354 0.16862746]\n   ...\n   [0.6666667  0.6117647  0.56078434]\n   [0.6627451  0.60784316 0.5568628 ]\n   [0.6666667  0.6117647  0.56078434]]\n\n  [[0.35686275 0.25882354 0.17254902]\n   [0.3254902  0.227451   0.14117648]\n   [0.34117648 0.2392157  0.14901961]\n   ...\n   [0.6666667  0.6117647  0.56078434]\n   [0.6666667  0.6117647  0.56078434]\n   [0.65882355 0.6039216  0.5529412 ]]]\n\n\n [[[0.48627454 0.54509807 0.5568628 ]\n   [0.45098042 0.5019608  0.53333336]\n   [0.3254902  0.37254903 0.41960788]\n   ...\n   [0.6039216  0.60784316 0.6156863 ]\n   [0.69803923 0.7019608  0.7176471 ]\n   [0.61960787 0.6156863  0.63529414]]\n\n  [[0.44705886 0.5058824  0.5176471 ]\n   [0.39607847 0.44705886 0.4784314 ]\n   [0.35686275 0.39607847 0.4431373 ]\n   ...\n   [0.50980395 0.5294118  0.54509807]\n   [0.4156863  0.427451   0.454902  ]\n   [0.5058824  0.5058824  0.5372549 ]]\n\n  [[0.4431373  0.49411768 0.5176471 ]\n   [0.34117648 0.3921569  0.42352945]\n   [0.3647059  0.4039216  0.4431373 ]\n   ...\n   [0.38431376 0.4156863  0.45882356]\n   [0.40000004 0.42352945 0.47058827]\n   [0.49803925 0.52156866 0.5686275 ]]\n\n  ...\n\n  [[0.14901961 0.14509805 0.12941177]\n   [0.16470589 0.16078432 0.14509805]\n   [0.15686275 0.15294118 0.13725491]\n   ...\n   [0.45098042 0.5294118  0.57254905]\n   [0.32156864 0.38823533 0.427451  ]\n   [0.41960788 0.45882356 0.49411768]]\n\n  [[0.18431373 0.18039216 0.16470589]\n   [0.16470589 0.16078432 0.14509805]\n   [0.14117648 0.13725491 0.12156864]\n   ...\n   [0.3921569  0.4784314  0.5294118 ]\n   [0.427451   0.49411768 0.53333336]\n   [0.37254903 0.40784317 0.4431373 ]]\n\n  [[0.18039216 0.1764706  0.16078432]\n   [0.16470589 0.16078432 0.14509805]\n   [0.17254902 0.16862746 0.15294118]\n   ...\n   [0.3372549  0.42352945 0.4784314 ]\n   [0.31764707 0.37254903 0.4156863 ]\n   [0.5019608  0.5294118  0.56078434]]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-efc2ba541ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Score trained model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sofia/Library/Python/2.7/lib/python/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m                 initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sofia/Library/Python/2.7/lib/python/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sofia/Library/Python/2.7/lib/python/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sofia/Library/Python/2.7/lib/python/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    200\u001b[0m                                      \u001b[0;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                                      \u001b[0;34m'or `(x, y)`. Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                                      str(generator_output))\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0;31m# Handle data tensors support when no input given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   ...\n   [0.22352943 0.21960786 0.20000002]\n   [0.21960786 0.21568629 0.19607845]\n   [0.21568629 0.21176472 0.19215688]]\n\n  [[0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   ...\n   [0.19607845 0.20000002 0.1764706 ]\n   [0.19607845 0.20000002 0.1764706 ]\n   [0.19607845 0.20000002 0.1764706 ]]\n\n  [[0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   [0.63529414 0.57254905 0.5137255 ]\n   ...\n   [0.18431373 0.18823531 0.16862746]\n   [0.17254902 0.1764706  0.15686275]\n   [0.17254902 0.1764706  0.15686275]]\n\n  ...\n\n  [[0.6862745  0.64705884 0.60784316]\n   [0.6862745  0.64705884 0.60784316]\n   [0.6862745  0.64705884 0.60784316]\n   ...\n   [0.29803923 0.35686275 0.38431376]\n   [0.2901961  0.34117648 0.37254903]\n   [0.28627452 0.3372549  0.36862746]]\n\n  [[0.68235296 0.6431373  0.6039216 ]\n   [0.6862745  0.64705884 0.60784316]\n   [0.6862745  0.64705884 0.60784316]\n   ...\n   [0.30588236 0.3647059  0.3921569 ]\n   [0.29411766 0.34509805 0.37647063]\n   [0.27450982 0.3254902  0.35686275]]\n\n  [[0.6862745  0.64705884 0.60784316]\n   [0.68235296 0.6431373  0.6039216 ]\n   [0.68235296 0.6431373  0.6039216 ]\n   ...\n   [0.2901961  0.34901962 0.37647063]\n   [0.28235295 0.33333334 0.3647059 ]\n   [0.2784314  0.32941177 0.36078432]]]\n\n\n [[[0.10196079 0.10196079 0.09411766]\n   [0.10980393 0.10980393 0.10196079]\n   [0.09803922 0.09803922 0.09019608]\n   ...\n   [0.39607847 0.25490198 0.12156864]\n   [0.4039216  0.2627451  0.12941177]\n   [0.427451   0.25882354 0.1254902 ]]\n\n  [[0.09803922 0.09803922 0.09019608]\n   [0.10588236 0.10588236 0.09803922]\n   [0.09803922 0.09803922 0.09019608]\n   ...\n   [0.427451   0.27450982 0.14509805]\n   [0.43529415 0.28235295 0.15294118]\n   [0.454902   0.2784314  0.14901961]]\n\n  [[0.09411766 0.09411766 0.08627451]\n   [0.09803922 0.09803922 0.09019608]\n   [0.09803922 0.09803922 0.09019608]\n   ...\n   [0.454902   0.28627452 0.14901961]\n   [0.45882356 0.2901961  0.15294118]\n   [0.47450984 0.29411766 0.15294118]]\n\n  ...\n\n  [[0.06666667 0.05882353 0.07058824]\n   [0.18039216 0.1764706  0.16862746]\n   [0.5686275  0.5411765  0.4784314 ]\n   ...\n   [0.5568628  0.43137258 0.24705884]\n   [0.5568628  0.43137258 0.24705884]\n   [0.5529412  0.43137258 0.24705884]]\n\n  [[0.06666667 0.05882353 0.07058824]\n   [0.18039216 0.1764706  0.16862746]\n   [0.56078434 0.53333336 0.47058827]\n   ...\n   [0.5568628  0.43137258 0.24705884]\n   [0.5568628  0.43137258 0.24705884]\n   [0.5529412  0.43137258 0.24705884]]\n\n  [[0.06666667 0.05882353 0.07058824]\n   [0.18039216 0.1764706  0.16862746]\n   [0.5529412  0.5254902  0.46274513]\n   ...\n   [0.5568628  0.43137258 0.24705884]\n   [0.5568628  0.43137258 0.24705884]\n   [0.5529412  0.43137258 0.24705884]]]\n\n\n [[[0.78823537 0.79215693 0.8000001 ]\n   [0.7960785  0.8000001  0.8078432 ]\n   [0.7843138  0.78823537 0.7960785 ]\n   ...\n   [0.73333335 0.74509805 0.76470596]\n   [0.74509805 0.7490196  0.7686275 ]\n   [0.74509805 0.7490196  0.7686275 ]]\n\n  [[0.7803922  0.7843138  0.79215693]\n   [0.7686275  0.7725491  0.7803922 ]\n   [0.78823537 0.79215693 0.8000001 ]\n   ...\n   [0.76470596 0.77647066 0.7960785 ]\n   [0.7568628  0.7607844  0.7803922 ]\n   [0.7568628  0.7607844  0.7803922 ]]\n\n  [[0.79215693 0.7960785  0.80392164]\n   [0.7960785  0.8000001  0.8078432 ]\n   [0.79215693 0.7960785  0.80392164]\n   ...\n   [0.73333335 0.74509805 0.76470596]\n   [0.7372549  0.7411765  0.7607844 ]\n   [0.7607844  0.76470596 0.7843138 ]]\n\n  ...\n\n  [[0.8352942  0.83921576 0.8196079 ]\n   [0.82745105 0.8313726  0.8000001 ]\n   [0.8352942  0.8431373  0.79215693]\n   ...\n   [0.7960785  0.81568635 0.8000001 ]\n   [0.7372549  0.7372549  0.7294118 ]\n   [0.7960785  0.7960785  0.78823537]]\n\n  [[0.85098046 0.85098046 0.8588236 ]\n   [0.85098046 0.85098046 0.8431373 ]\n   [0.85098046 0.8588236  0.81568635]\n   ...\n   [0.79215693 0.8117648  0.7960785 ]\n   [0.79215693 0.79215693 0.7843138 ]\n   [0.8078432  0.8078432  0.8000001 ]]\n\n  [[0.81568635 0.8196079  0.78823537]\n   [0.8235295  0.82745105 0.80392164]\n   [0.86666673 0.86666673 0.8588236 ]\n   ...\n   [0.8078432  0.7843138  0.7843138 ]\n   [0.81568635 0.8117648  0.7960785 ]\n   [0.8000001  0.7960785  0.7803922 ]]]\n\n\n ...\n\n\n [[[0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   ...\n   [0.04705883 0.01568628 0.00784314]\n   [0.04313726 0.01176471 0.00392157]\n   [0.04705883 0.         0.        ]]\n\n  [[0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   ...\n   [0.03921569 0.         0.        ]\n   [0.03529412 0.         0.        ]\n   [0.04705883 0.         0.        ]]\n\n  [[0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   [0.01176471 0.00392157 0.00784314]\n   ...\n   [0.0509804  0.01176471 0.00784314]\n   [0.04705883 0.00784314 0.00392157]\n   [0.04705883 0.         0.        ]]\n\n  ...\n\n  [[0.41176474 0.38823533 0.38823533]\n   [0.32156864 0.29803923 0.29803923]\n   [0.38823533 0.36862746 0.35686275]\n   ...\n   [0.58431375 0.62352943 0.5882353 ]\n   [0.5921569  0.6156863  0.6       ]\n   [0.5254902  0.5411765  0.5529412 ]]\n\n  [[0.36078432 0.34117648 0.32941177]\n   [0.2784314  0.25490198 0.25490198]\n   [0.36078432 0.3372549  0.34509805]\n   ...\n   [0.59607846 0.63529414 0.5921569 ]\n   [0.5882353  0.6039216  0.6       ]\n   [0.5411765  0.5529412  0.57254905]]\n\n  [[0.40784317 0.38823533 0.37254903]\n   [0.35686275 0.33333334 0.33333334]\n   [0.36862746 0.34509805 0.36078432]\n   ...\n   [0.5764706  0.6039216  0.57254905]\n   [0.53333336 0.5372549  0.5529412 ]\n   [0.47450984 0.48627454 0.5137255 ]]]\n\n\n [[[0.8196079  0.8235295  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   ...\n   [0.7686275  0.7568628  0.63529414]\n   [0.76470596 0.75294125 0.6313726 ]\n   [0.76470596 0.75294125 0.6313726 ]]\n\n  [[0.8235295  0.8196079  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   [0.8235295  0.8196079  0.69803923]\n   ...\n   [0.7686275  0.7568628  0.63529414]\n   [0.7686275  0.7568628  0.63529414]\n   [0.76470596 0.75294125 0.6313726 ]]\n\n  [[0.8235295  0.8196079  0.7019608 ]\n   [0.8235295  0.8196079  0.7019608 ]\n   [0.8235295  0.8196079  0.7019608 ]\n   ...\n   [0.7686275  0.7568628  0.63529414]\n   [0.7686275  0.7568628  0.63529414]\n   [0.7686275  0.7568628  0.63529414]]\n\n  ...\n\n  [[0.35686275 0.25882354 0.17254902]\n   [0.37254903 0.27450982 0.18823531]\n   [0.41960788 0.31764707 0.227451  ]\n   ...\n   [0.654902   0.6        0.54901963]\n   [0.654902   0.6        0.54901963]\n   [0.65882355 0.6039216  0.5529412 ]]\n\n  [[0.35686275 0.25882354 0.17254902]\n   [0.3372549  0.2392157  0.15294118]\n   [0.36078432 0.25882354 0.16862746]\n   ...\n   [0.6666667  0.6117647  0.56078434]\n   [0.6627451  0.60784316 0.5568628 ]\n   [0.6666667  0.6117647  0.56078434]]\n\n  [[0.35686275 0.25882354 0.17254902]\n   [0.3254902  0.227451   0.14117648]\n   [0.34117648 0.2392157  0.14901961]\n   ...\n   [0.6666667  0.6117647  0.56078434]\n   [0.6666667  0.6117647  0.56078434]\n   [0.65882355 0.6039216  0.5529412 ]]]\n\n\n [[[0.48627454 0.54509807 0.5568628 ]\n   [0.45098042 0.5019608  0.53333336]\n   [0.3254902  0.37254903 0.41960788]\n   ...\n   [0.6039216  0.60784316 0.6156863 ]\n   [0.69803923 0.7019608  0.7176471 ]\n   [0.61960787 0.6156863  0.63529414]]\n\n  [[0.44705886 0.5058824  0.5176471 ]\n   [0.39607847 0.44705886 0.4784314 ]\n   [0.35686275 0.39607847 0.4431373 ]\n   ...\n   [0.50980395 0.5294118  0.54509807]\n   [0.4156863  0.427451   0.454902  ]\n   [0.5058824  0.5058824  0.5372549 ]]\n\n  [[0.4431373  0.49411768 0.5176471 ]\n   [0.34117648 0.3921569  0.42352945]\n   [0.3647059  0.4039216  0.4431373 ]\n   ...\n   [0.38431376 0.4156863  0.45882356]\n   [0.40000004 0.42352945 0.47058827]\n   [0.49803925 0.52156866 0.5686275 ]]\n\n  ...\n\n  [[0.14901961 0.14509805 0.12941177]\n   [0.16470589 0.16078432 0.14509805]\n   [0.15686275 0.15294118 0.13725491]\n   ...\n   [0.45098042 0.5294118  0.57254905]\n   [0.32156864 0.38823533 0.427451  ]\n   [0.41960788 0.45882356 0.49411768]]\n\n  [[0.18431373 0.18039216 0.16470589]\n   [0.16470589 0.16078432 0.14509805]\n   [0.14117648 0.13725491 0.12156864]\n   ...\n   [0.3921569  0.4784314  0.5294118 ]\n   [0.427451   0.49411768 0.53333336]\n   [0.37254903 0.40784317 0.4431373 ]]\n\n  [[0.18039216 0.1764706  0.16078432]\n   [0.16470589 0.16078432 0.14509805]\n   [0.17254902 0.16862746 0.15294118]\n   ...\n   [0.3372549  0.42352945 0.4784314 ]\n   [0.31764707 0.37254903 0.4156863 ]\n   [0.5019608  0.5294118  0.56078434]]]]"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, validation_data=validation_generator)\n",
    "    \n",
    "\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = model.history.history['accuracy']\n",
    "val_acc = model.history.history['val_accuracy']\n",
    "loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "\n",
    "\n",
    "epochs=range(1,len(loss)+1)\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
